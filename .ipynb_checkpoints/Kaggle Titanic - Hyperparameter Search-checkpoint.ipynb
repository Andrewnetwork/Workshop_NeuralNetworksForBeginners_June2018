{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\huge\\text{Kaggle Titanic: Hyperparameter Search} $$\n",
    "\n",
    "$$ \\small\\text{Titanic: Machine Learning from Disaster}$$\n",
    "\n",
    "$$ \\large\\text{Andrew Ribeiro - June 2018 -  Andrew@kexp.io }$$\n",
    "![Titanic](notebook_images/4679-004-8C0793EF.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\Anaconda3\\envs\\GreatEnv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# General Libraries \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "    \n",
    "# Scikit-Learn Libraries\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras Libraries \n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "# Interactive Widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Load Data\n",
    "trainDF = pd.read_csv(\"./data/titanic/train.csv\")\n",
    "testDF = pd.read_csv(\"./data/titanic/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanBaseline(df):\n",
    "    nRows = df[\"Survived\"].shape[0]\n",
    "    mean = df[\"Survived\"].sum()/nRows\n",
    "    return log_loss(df[\"Survived\"],np.full(nRows,mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113.2750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.5875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4     5    6    7         8    9    10   11\n",
       "0  0.0  0.0  1.0  1.0  0.0  15.0  0.0  1.0  211.3375  0.0  0.0  1.0\n",
       "1  0.0  1.0  0.0  1.0  0.0  33.0  1.0  2.0   27.7500  0.0  0.0  1.0\n",
       "2  1.0  0.0  0.0  0.0  1.0  22.0  0.0  0.0    7.2250  0.0  1.0  0.0\n",
       "3  0.0  0.0  1.0  0.0  1.0  58.0  0.0  2.0  113.2750  0.0  1.0  0.0\n",
       "4  0.0  0.0  1.0  0.0  1.0  47.0  0.0  0.0   25.5875  0.0  0.0  1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our prepprocessing function which is applied to every row of the target dataframe. \n",
    "def preprocessRow(row):\n",
    "    # Process Categorical Variables - One-Hot-Encoding\n",
    "    sex      = [0,0]\n",
    "    embarked = [0,0,0]\n",
    "    pclass   = [0,0,0]\n",
    "    \n",
    "    if row[\"Sex\"] == \"male\":\n",
    "        sex = [0,1]\n",
    "    elif row[\"Sex\"] == \"female\":\n",
    "        sex = [1,0]\n",
    "    \n",
    "    if row[\"Embarked\"] == \"S\":\n",
    "        embarked = [0,0,1]\n",
    "    elif row[\"Embarked\"] == \"C\":\n",
    "        embarked = [0,1,0]\n",
    "    elif row[\"Embarked\"] == \"Q\":\n",
    "        embarked = [1,0,0]\n",
    "    \n",
    "    if row[\"Pclass\"] == 1:\n",
    "        pclass   = [0,0,1]\n",
    "    elif row[\"Pclass\"] == 2:\n",
    "        pclass   = [0,1,0]\n",
    "    elif row[\"Pclass\"] == 3:\n",
    "        pclass   = [1,0,0]\n",
    " \n",
    "    return pclass+sex+[row[\"Age\"],row[\"SibSp\"],row[\"Parch\"],row[\"Fare\"]]+embarked\n",
    "\n",
    "# Fill Missing Values\n",
    "testDF = testDF.fillna(0)\n",
    "trainDF = trainDF.fillna(0).sample(frac=1)\n",
    "\n",
    "# Preprocess Data\n",
    "data = np.stack(trainDF.apply(preprocessRow,axis=1).as_matrix())\n",
    "\n",
    "# View what the training vectors look like. \n",
    "pd.DataFrame(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, validation_x, train_y , validation_y = train_test_split(data, trainDF[\"Survived\"].as_matrix(), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "batch_size = 300\n",
    "epochs = 600\n",
    "\n",
    "# Model Hyperparameters \n",
    "lossFn = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "\n",
    "# Model Architecture \n",
    "def trainModel(nLayers,layerSize,dropoutPercent,lossFn,optimizer,batch_size,epochs):\n",
    "    K.clear_session()\n",
    "        \n",
    "    inputs = Input(shape=(train_x.shape[1],),name=\"input\")\n",
    "    x = None \n",
    "    \n",
    "    for layer in range(nLayers):\n",
    "        if x == None:\n",
    "            x = inputs\n",
    "\n",
    "        x = Dense(layerSize, activation='sigmoid',name=\"fc\"+str(layer))(x)\n",
    "        x = Dropout(dropoutPercent,name=\"fc_dropout_\"+str(layer))(x)\n",
    "\n",
    "    out = Dense(1,activation='sigmoid', name=\"output\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=lossFn,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    hist = model.fit(train_x, train_y,validation_data=(validation_x,validation_y), \n",
    "                     batch_size=batch_size,epochs=epochs, verbose=0)\n",
    "\n",
    "    del model \n",
    "    gc.collect()\n",
    "    \n",
    "    return (nLayers,layerSize,dropoutPercent,lossFn,optimizer,batch_size,epochs),hist\n",
    "\n",
    "\n",
    "def gridSearch(layersGrid,layerSizeGrid,dropoutGrid,batchGrid):\n",
    "    results = {}\n",
    "    for nLayers in layersGrid:\n",
    "        for layerSize in layerSizeGrid:\n",
    "            for dropoutPercent in dropoutGrid:\n",
    "                for batch_size in batchGrid:\n",
    "                    idx,hist = trainModel(nLayers,layerSize,dropoutPercent,lossFn,optimizer,batch_size,epochs)\n",
    "                    results[idx] = hist\n",
    "\n",
    "                    loss = hist.history['loss'][-1]\n",
    "                    val_loss = hist.history['val_loss'][-1]\n",
    "                    print(\"{0:3} x {1:3} Layers with {2:2f} dropout and batch size of {3:3} | {4:5f} @ {5:5f}%\"\n",
    "                          .format(nLayers,layerSize,dropoutPercent,batch_size,loss,np.abs((1 - (loss/val_loss))*100)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 x   5 Layers with 0.000000 dropout and batch size of  50 | 0.392815 @ 18.441103%\n",
      "  1 x   5 Layers with 0.000000 dropout and batch size of 100 | 0.412536 @ 16.291910%\n",
      "  1 x   5 Layers with 0.000000 dropout and batch size of 150 | 0.411545 @ 14.620942%\n",
      "  1 x   5 Layers with 0.000000 dropout and batch size of 200 | 0.424532 @ 13.556138%\n",
      "  1 x   5 Layers with 0.000000 dropout and batch size of 250 | 0.434791 @ 12.706046%\n",
      "  1 x   5 Layers with 0.000000 dropout and batch size of 300 | 0.439519 @ 11.832256%\n",
      "  1 x   5 Layers with 0.000000 dropout and batch size of 350 | 0.483606 @ 9.589988%\n",
      "  1 x   5 Layers with 0.100000 dropout and batch size of  50 | 0.407524 @ 11.900760%\n",
      "  1 x   5 Layers with 0.100000 dropout and batch size of 100 | 0.421880 @ 12.041523%\n",
      "  1 x   5 Layers with 0.100000 dropout and batch size of 150 | 0.426762 @ 10.850315%\n",
      "  1 x   5 Layers with 0.100000 dropout and batch size of 200 | 0.468927 @ 7.494878%\n",
      "  1 x   5 Layers with 0.100000 dropout and batch size of 250 | 0.476856 @ 5.782232%\n",
      "  1 x   5 Layers with 0.100000 dropout and batch size of 300 | 0.459251 @ 7.391412%\n",
      "  1 x   5 Layers with 0.100000 dropout and batch size of 350 | 0.463697 @ 8.684011%\n",
      "  1 x   5 Layers with 0.200000 dropout and batch size of  50 | 0.444577 @ 5.749378%\n",
      "  1 x   5 Layers with 0.200000 dropout and batch size of 100 | 0.444121 @ 7.164087%\n",
      "  1 x   5 Layers with 0.200000 dropout and batch size of 150 | 0.451434 @ 7.177583%\n",
      "  1 x   5 Layers with 0.200000 dropout and batch size of 200 | 0.481494 @ 2.830162%\n",
      "  1 x   5 Layers with 0.200000 dropout and batch size of 250 | 0.483601 @ 6.370638%\n",
      "  1 x   5 Layers with 0.200000 dropout and batch size of 300 | 0.458990 @ 8.432762%\n",
      "  1 x   5 Layers with 0.200000 dropout and batch size of 350 | 0.499778 @ 3.605602%\n",
      "  1 x   5 Layers with 0.300000 dropout and batch size of  50 | 0.452963 @ 4.106538%\n",
      "  1 x   5 Layers with 0.300000 dropout and batch size of 100 | 0.446582 @ 6.883561%\n",
      "  1 x   5 Layers with 0.300000 dropout and batch size of 150 | 0.491237 @ 2.339246%\n",
      "  1 x   5 Layers with 0.300000 dropout and batch size of 200 | 0.471180 @ 8.006650%\n",
      "  1 x   5 Layers with 0.300000 dropout and batch size of 250 | 0.509090 @ 2.241396%\n",
      "  1 x   5 Layers with 0.300000 dropout and batch size of 300 | 0.484666 @ 2.527621%\n",
      "  1 x   5 Layers with 0.300000 dropout and batch size of 350 | 0.534242 @ 0.067552%\n",
      "  1 x   5 Layers with 0.400000 dropout and batch size of  50 | 0.464645 @ 3.945844%\n",
      "  1 x   5 Layers with 0.400000 dropout and batch size of 100 | 0.537743 @ 1.911880%\n",
      "  1 x   5 Layers with 0.400000 dropout and batch size of 150 | 0.481309 @ 1.120483%\n",
      "  1 x   5 Layers with 0.400000 dropout and batch size of 200 | 0.486057 @ 3.103984%\n",
      "  1 x   5 Layers with 0.400000 dropout and batch size of 250 | 0.506987 @ 0.711125%\n",
      "  1 x   5 Layers with 0.400000 dropout and batch size of 300 | 0.540350 @ 3.736672%\n",
      "  1 x   5 Layers with 0.400000 dropout and batch size of 350 | 0.625056 @ 1.007709%\n",
      "  1 x   5 Layers with 0.500000 dropout and batch size of  50 | 0.497986 @ 6.664138%\n",
      "  1 x   5 Layers with 0.500000 dropout and batch size of 100 | 0.512013 @ 2.447649%\n",
      "  1 x   5 Layers with 0.500000 dropout and batch size of 150 | 0.530024 @ 1.413456%\n",
      "  1 x   5 Layers with 0.500000 dropout and batch size of 200 | 0.563631 @ 0.227525%\n",
      "  1 x   5 Layers with 0.500000 dropout and batch size of 250 | 0.517781 @ 0.907794%\n",
      "  1 x   5 Layers with 0.500000 dropout and batch size of 300 | 0.577565 @ 0.092358%\n",
      "  1 x   5 Layers with 0.500000 dropout and batch size of 350 | 0.597954 @ 0.329329%\n",
      "  1 x   5 Layers with 0.600000 dropout and batch size of  50 | 0.555609 @ 11.674160%\n",
      "  1 x   5 Layers with 0.600000 dropout and batch size of 100 | 0.568392 @ 3.486974%\n",
      "  1 x   5 Layers with 0.600000 dropout and batch size of 150 | 0.584530 @ 0.893599%\n",
      "  1 x   5 Layers with 0.600000 dropout and batch size of 200 | 0.568495 @ 1.355873%\n",
      "  1 x   5 Layers with 0.600000 dropout and batch size of 250 | 0.559228 @ 2.511245%\n",
      "  1 x   5 Layers with 0.600000 dropout and batch size of 300 | 0.563565 @ 6.491428%\n",
      "  1 x   5 Layers with 0.600000 dropout and batch size of 350 | 0.580993 @ 7.956489%\n",
      "  1 x   5 Layers with 0.700000 dropout and batch size of  50 | 0.545778 @ 7.011468%\n",
      "  1 x   5 Layers with 0.700000 dropout and batch size of 100 | 0.561101 @ 6.011278%\n",
      "  1 x   5 Layers with 0.700000 dropout and batch size of 150 | 0.582167 @ 8.345021%\n",
      "  1 x   5 Layers with 0.700000 dropout and batch size of 200 | 0.584086 @ 7.024705%\n",
      "  1 x   5 Layers with 0.700000 dropout and batch size of 250 | 0.604529 @ 2.027895%\n",
      "  1 x   5 Layers with 0.700000 dropout and batch size of 300 | 0.609630 @ 1.407922%\n",
      "  1 x   5 Layers with 0.700000 dropout and batch size of 350 | 0.617033 @ 1.969293%\n",
      "  1 x   5 Layers with 0.800000 dropout and batch size of  50 | 0.570649 @ 4.934326%\n",
      "  1 x   5 Layers with 0.800000 dropout and batch size of 100 | 0.594082 @ 2.434982%\n",
      "  1 x   5 Layers with 0.800000 dropout and batch size of 150 | 0.579244 @ 3.305073%\n",
      "  1 x   5 Layers with 0.800000 dropout and batch size of 200 | 0.620251 @ 1.327947%\n",
      "  1 x   5 Layers with 0.800000 dropout and batch size of 250 | 0.622251 @ 1.709675%\n",
      "  1 x   5 Layers with 0.800000 dropout and batch size of 300 | 0.618448 @ 0.922692%\n",
      "  1 x   5 Layers with 0.800000 dropout and batch size of 350 | 0.639978 @ 0.461892%\n",
      "  1 x   5 Layers with 0.900000 dropout and batch size of  50 | 0.630047 @ 3.040973%\n",
      "  1 x   5 Layers with 0.900000 dropout and batch size of 100 | 0.638536 @ 1.852183%\n",
      "  1 x   5 Layers with 0.900000 dropout and batch size of 150 | 0.646280 @ 0.102425%\n",
      "  1 x   5 Layers with 0.900000 dropout and batch size of 200 | 0.653890 @ 2.019425%\n",
      "  1 x   5 Layers with 0.900000 dropout and batch size of 250 | 0.640302 @ 0.074239%\n",
      "  1 x   5 Layers with 0.900000 dropout and batch size of 300 | 0.655935 @ 1.920513%\n",
      "  1 x   5 Layers with 0.900000 dropout and batch size of 350 | 0.656231 @ 1.368437%\n",
      "  1 x  35 Layers with 0.000000 dropout and batch size of  50 | 0.352798 @ 24.739661%\n",
      "  1 x  35 Layers with 0.000000 dropout and batch size of 100 | 0.369068 @ 20.235185%\n",
      "  1 x  35 Layers with 0.000000 dropout and batch size of 150 | 0.367271 @ 20.295670%\n",
      "  1 x  35 Layers with 0.000000 dropout and batch size of 200 | 0.379238 @ 18.324992%\n",
      "  1 x  35 Layers with 0.000000 dropout and batch size of 250 | 0.392915 @ 17.662665%\n",
      "  1 x  35 Layers with 0.000000 dropout and batch size of 300 | 0.395771 @ 18.494098%\n",
      "  1 x  35 Layers with 0.000000 dropout and batch size of 350 | 0.425307 @ 13.332322%\n",
      "  1 x  35 Layers with 0.100000 dropout and batch size of  50 | 0.360244 @ 21.343980%\n",
      "  1 x  35 Layers with 0.100000 dropout and batch size of 100 | 0.395179 @ 16.381273%\n",
      "  1 x  35 Layers with 0.100000 dropout and batch size of 150 | 0.391835 @ 15.839176%\n",
      "  1 x  35 Layers with 0.100000 dropout and batch size of 200 | 0.387343 @ 17.397005%\n",
      "  1 x  35 Layers with 0.100000 dropout and batch size of 250 | 0.409273 @ 13.612450%\n",
      "  1 x  35 Layers with 0.100000 dropout and batch size of 300 | 0.411186 @ 13.538447%\n",
      "  1 x  35 Layers with 0.100000 dropout and batch size of 350 | 0.437937 @ 11.493452%\n",
      "  1 x  35 Layers with 0.200000 dropout and batch size of  50 | 0.381474 @ 17.830370%\n",
      "  1 x  35 Layers with 0.200000 dropout and batch size of 100 | 0.404762 @ 14.142647%\n",
      "  1 x  35 Layers with 0.200000 dropout and batch size of 150 | 0.408328 @ 12.628515%\n",
      "  1 x  35 Layers with 0.200000 dropout and batch size of 200 | 0.411819 @ 12.248693%\n",
      "  1 x  35 Layers with 0.200000 dropout and batch size of 250 | 0.404627 @ 15.459816%\n",
      "  1 x  35 Layers with 0.200000 dropout and batch size of 300 | 0.408891 @ 14.393644%\n",
      "  1 x  35 Layers with 0.200000 dropout and batch size of 350 | 0.454218 @ 7.804748%\n",
      "  1 x  35 Layers with 0.300000 dropout and batch size of  50 | 0.393864 @ 16.566721%\n",
      "  1 x  35 Layers with 0.300000 dropout and batch size of 100 | 0.412166 @ 13.948712%\n",
      "  1 x  35 Layers with 0.300000 dropout and batch size of 150 | 0.416830 @ 10.518142%\n",
      "  1 x  35 Layers with 0.300000 dropout and batch size of 200 | 0.415154 @ 11.345171%\n",
      "  1 x  35 Layers with 0.300000 dropout and batch size of 250 | 0.430253 @ 8.970935%\n",
      "  1 x  35 Layers with 0.300000 dropout and batch size of 300 | 0.436332 @ 7.582856%\n",
      "  1 x  35 Layers with 0.300000 dropout and batch size of 350 | 0.460682 @ 6.520218%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 x  35 Layers with 0.400000 dropout and batch size of  50 | 0.400410 @ 15.403421%\n",
      "  1 x  35 Layers with 0.400000 dropout and batch size of 100 | 0.413086 @ 12.643614%\n",
      "  1 x  35 Layers with 0.400000 dropout and batch size of 150 | 0.405049 @ 13.651195%\n",
      "  1 x  35 Layers with 0.400000 dropout and batch size of 200 | 0.432729 @ 8.298724%\n",
      "  1 x  35 Layers with 0.400000 dropout and batch size of 250 | 0.441281 @ 6.672898%\n",
      "  1 x  35 Layers with 0.400000 dropout and batch size of 300 | 0.428403 @ 10.805376%\n",
      "  1 x  35 Layers with 0.400000 dropout and batch size of 350 | 0.452865 @ 8.036481%\n",
      "  1 x  35 Layers with 0.500000 dropout and batch size of  50 | 0.409717 @ 13.245347%\n",
      "  1 x  35 Layers with 0.500000 dropout and batch size of 100 | 0.427299 @ 10.226480%\n",
      "  1 x  35 Layers with 0.500000 dropout and batch size of 150 | 0.419957 @ 11.915401%\n",
      "  1 x  35 Layers with 0.500000 dropout and batch size of 200 | 0.431920 @ 8.822403%\n",
      "  1 x  35 Layers with 0.500000 dropout and batch size of 250 | 0.436120 @ 7.992251%\n",
      "  1 x  35 Layers with 0.500000 dropout and batch size of 300 | 0.449040 @ 5.880480%\n",
      "  1 x  35 Layers with 0.500000 dropout and batch size of 350 | 0.477476 @ 2.011878%\n",
      "  1 x  35 Layers with 0.600000 dropout and batch size of  50 | 0.435496 @ 7.370765%\n",
      "  1 x  35 Layers with 0.600000 dropout and batch size of 100 | 0.432623 @ 8.859555%\n",
      "  1 x  35 Layers with 0.600000 dropout and batch size of 150 | 0.428585 @ 9.157639%\n",
      "  1 x  35 Layers with 0.600000 dropout and batch size of 200 | 0.436470 @ 7.251829%\n",
      "  1 x  35 Layers with 0.600000 dropout and batch size of 250 | 0.461597 @ 3.379305%\n",
      "  1 x  35 Layers with 0.600000 dropout and batch size of 300 | 0.468061 @ 3.979134%\n",
      "  1 x  35 Layers with 0.600000 dropout and batch size of 350 | 0.485433 @ 0.830431%\n",
      "  1 x  35 Layers with 0.700000 dropout and batch size of  50 | 0.442140 @ 5.819550%\n",
      "  1 x  35 Layers with 0.700000 dropout and batch size of 100 | 0.438586 @ 6.669091%\n",
      "  1 x  35 Layers with 0.700000 dropout and batch size of 150 | 0.450472 @ 5.484968%\n",
      "  1 x  35 Layers with 0.700000 dropout and batch size of 200 | 0.463553 @ 2.128416%\n",
      "  1 x  35 Layers with 0.700000 dropout and batch size of 250 | 0.475926 @ 2.314607%\n",
      "  1 x  35 Layers with 0.700000 dropout and batch size of 300 | 0.473666 @ 2.868580%\n",
      "  1 x  35 Layers with 0.700000 dropout and batch size of 350 | 0.483471 @ 3.745864%\n",
      "  1 x  35 Layers with 0.800000 dropout and batch size of  50 | 0.477213 @ 1.180269%\n",
      "  1 x  35 Layers with 0.800000 dropout and batch size of 100 | 0.463130 @ 2.762891%\n",
      "  1 x  35 Layers with 0.800000 dropout and batch size of 150 | 0.496585 @ 3.668859%\n",
      "  1 x  35 Layers with 0.800000 dropout and batch size of 200 | 0.494471 @ 1.606600%\n",
      "  1 x  35 Layers with 0.800000 dropout and batch size of 250 | 0.488881 @ 0.702962%\n",
      "  1 x  35 Layers with 0.800000 dropout and batch size of 300 | 0.497809 @ 0.866267%\n",
      "  1 x  35 Layers with 0.800000 dropout and batch size of 350 | 0.541301 @ 3.848007%\n",
      "  1 x  35 Layers with 0.900000 dropout and batch size of  50 | 0.514032 @ 7.302359%\n",
      "  1 x  35 Layers with 0.900000 dropout and batch size of 100 | 0.554269 @ 11.357978%\n",
      "  1 x  35 Layers with 0.900000 dropout and batch size of 150 | 0.523183 @ 3.882980%\n",
      "  1 x  35 Layers with 0.900000 dropout and batch size of 200 | 0.524345 @ 2.545797%\n",
      "  1 x  35 Layers with 0.900000 dropout and batch size of 250 | 0.551861 @ 1.661566%\n",
      "  1 x  35 Layers with 0.900000 dropout and batch size of 300 | 0.579583 @ 6.418947%\n",
      "  1 x  35 Layers with 0.900000 dropout and batch size of 350 | 0.609086 @ 5.018728%\n",
      "  1 x  65 Layers with 0.000000 dropout and batch size of  50 | 0.332260 @ 28.998822%\n",
      "  1 x  65 Layers with 0.000000 dropout and batch size of 100 | 0.363852 @ 21.955850%\n",
      "  1 x  65 Layers with 0.000000 dropout and batch size of 150 | 0.360878 @ 21.126931%\n",
      "  1 x  65 Layers with 0.000000 dropout and batch size of 200 | 0.369036 @ 19.809251%\n",
      "  1 x  65 Layers with 0.000000 dropout and batch size of 250 | 0.381201 @ 18.494896%\n",
      "  1 x  65 Layers with 0.000000 dropout and batch size of 300 | 0.387061 @ 18.014769%\n",
      "  1 x  65 Layers with 0.000000 dropout and batch size of 350 | 0.410021 @ 15.292243%\n",
      "  1 x  65 Layers with 0.100000 dropout and batch size of  50 | 0.372278 @ 19.115952%\n",
      "  1 x  65 Layers with 0.100000 dropout and batch size of 100 | 0.388949 @ 16.689226%\n",
      "  1 x  65 Layers with 0.100000 dropout and batch size of 150 | 0.377493 @ 17.380439%\n",
      "  1 x  65 Layers with 0.100000 dropout and batch size of 200 | 0.393497 @ 14.631310%\n",
      "  1 x  65 Layers with 0.100000 dropout and batch size of 250 | 0.395648 @ 16.009934%\n",
      "  1 x  65 Layers with 0.100000 dropout and batch size of 300 | 0.411001 @ 13.958690%\n",
      "  1 x  65 Layers with 0.100000 dropout and batch size of 350 | 0.411454 @ 15.974577%\n",
      "  1 x  65 Layers with 0.200000 dropout and batch size of  50 | 0.380528 @ 17.465244%\n",
      "  1 x  65 Layers with 0.200000 dropout and batch size of 100 | 0.390396 @ 17.158505%\n",
      "  1 x  65 Layers with 0.200000 dropout and batch size of 150 | 0.389497 @ 16.194615%\n",
      "  1 x  65 Layers with 0.200000 dropout and batch size of 200 | 0.391696 @ 15.672425%\n",
      "  1 x  65 Layers with 0.200000 dropout and batch size of 250 | 0.404588 @ 13.319090%\n",
      "  1 x  65 Layers with 0.200000 dropout and batch size of 300 | 0.397995 @ 14.753889%\n",
      "  1 x  65 Layers with 0.200000 dropout and batch size of 350 | 0.439324 @ 11.284104%\n",
      "  1 x  65 Layers with 0.300000 dropout and batch size of  50 | 0.372031 @ 18.931772%\n",
      "  1 x  65 Layers with 0.300000 dropout and batch size of 100 | 0.402043 @ 13.890595%\n",
      "  1 x  65 Layers with 0.300000 dropout and batch size of 150 | 0.407490 @ 13.139724%\n",
      "  1 x  65 Layers with 0.300000 dropout and batch size of 200 | 0.406468 @ 13.089069%\n",
      "  1 x  65 Layers with 0.300000 dropout and batch size of 250 | 0.419837 @ 11.916284%\n",
      "  1 x  65 Layers with 0.300000 dropout and batch size of 300 | 0.432534 @ 9.313439%\n",
      "  1 x  65 Layers with 0.300000 dropout and batch size of 350 | 0.440943 @ 10.498677%\n",
      "  1 x  65 Layers with 0.400000 dropout and batch size of  50 | 0.400863 @ 14.184972%\n",
      "  1 x  65 Layers with 0.400000 dropout and batch size of 100 | 0.405550 @ 14.712747%\n",
      "  1 x  65 Layers with 0.400000 dropout and batch size of 150 | 0.407517 @ 12.396985%\n",
      "  1 x  65 Layers with 0.400000 dropout and batch size of 200 | 0.414541 @ 11.347685%\n",
      "  1 x  65 Layers with 0.400000 dropout and batch size of 250 | 0.414482 @ 12.024755%\n",
      "  1 x  65 Layers with 0.400000 dropout and batch size of 300 | 0.423737 @ 10.301350%\n",
      "  1 x  65 Layers with 0.400000 dropout and batch size of 350 | 0.462027 @ 5.719787%\n",
      "  1 x  65 Layers with 0.500000 dropout and batch size of  50 | 0.407376 @ 11.482200%\n",
      "  1 x  65 Layers with 0.500000 dropout and batch size of 100 | 0.415512 @ 10.707848%\n",
      "  1 x  65 Layers with 0.500000 dropout and batch size of 150 | 0.425388 @ 9.610115%\n",
      "  1 x  65 Layers with 0.500000 dropout and batch size of 200 | 0.441964 @ 7.699053%\n",
      "  1 x  65 Layers with 0.500000 dropout and batch size of 250 | 0.437465 @ 8.422879%\n",
      "  1 x  65 Layers with 0.500000 dropout and batch size of 300 | 0.436971 @ 8.517377%\n",
      "  1 x  65 Layers with 0.500000 dropout and batch size of 350 | 0.459661 @ 6.124119%\n",
      "  1 x  65 Layers with 0.600000 dropout and batch size of  50 | 0.415990 @ 11.477378%\n",
      "  1 x  65 Layers with 0.600000 dropout and batch size of 100 | 0.423514 @ 9.965273%\n",
      "  1 x  65 Layers with 0.600000 dropout and batch size of 150 | 0.430243 @ 8.859054%\n",
      "  1 x  65 Layers with 0.600000 dropout and batch size of 200 | 0.450899 @ 4.977037%\n",
      "  1 x  65 Layers with 0.600000 dropout and batch size of 250 | 0.446033 @ 5.906396%\n",
      "  1 x  65 Layers with 0.600000 dropout and batch size of 300 | 0.442927 @ 6.871036%\n",
      "  1 x  65 Layers with 0.600000 dropout and batch size of 350 | 0.475281 @ 3.227261%\n",
      "  1 x  65 Layers with 0.700000 dropout and batch size of  50 | 0.412314 @ 12.462162%\n",
      "  1 x  65 Layers with 0.700000 dropout and batch size of 100 | 0.426200 @ 10.078252%\n",
      "  1 x  65 Layers with 0.700000 dropout and batch size of 150 | 0.442235 @ 6.847863%\n",
      "  1 x  65 Layers with 0.700000 dropout and batch size of 200 | 0.437090 @ 7.730696%\n",
      "  1 x  65 Layers with 0.700000 dropout and batch size of 250 | 0.461864 @ 3.282889%\n",
      "  1 x  65 Layers with 0.700000 dropout and batch size of 300 | 0.444503 @ 6.897272%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 x  65 Layers with 0.700000 dropout and batch size of 350 | 0.499747 @ 1.438360%\n",
      "  1 x  65 Layers with 0.800000 dropout and batch size of  50 | 0.430211 @ 8.245997%\n",
      "  1 x  65 Layers with 0.800000 dropout and batch size of 100 | 0.446561 @ 5.600694%\n",
      "  1 x  65 Layers with 0.800000 dropout and batch size of 150 | 0.453469 @ 4.726603%\n",
      "  1 x  65 Layers with 0.800000 dropout and batch size of 200 | 0.459255 @ 3.319126%\n",
      "  1 x  65 Layers with 0.800000 dropout and batch size of 250 | 0.458033 @ 4.622236%\n",
      "  1 x  65 Layers with 0.800000 dropout and batch size of 300 | 0.472685 @ 3.010510%\n",
      "  1 x  65 Layers with 0.800000 dropout and batch size of 350 | 0.499416 @ 0.047587%\n",
      "  1 x  65 Layers with 0.900000 dropout and batch size of  50 | 0.479832 @ 1.598962%\n",
      "  1 x  65 Layers with 0.900000 dropout and batch size of 100 | 0.504463 @ 4.433264%\n",
      "  1 x  65 Layers with 0.900000 dropout and batch size of 150 | 0.506842 @ 3.812713%\n",
      "  1 x  65 Layers with 0.900000 dropout and batch size of 200 | 0.516781 @ 6.094191%\n",
      "  1 x  65 Layers with 0.900000 dropout and batch size of 250 | 0.508296 @ 2.859877%\n",
      "  1 x  65 Layers with 0.900000 dropout and batch size of 300 | 0.500663 @ 0.302247%\n",
      "  1 x  65 Layers with 0.900000 dropout and batch size of 350 | 0.540520 @ 0.613916%\n",
      "  1 x  95 Layers with 0.000000 dropout and batch size of  50 | 0.327055 @ 31.009931%\n",
      "  1 x  95 Layers with 0.000000 dropout and batch size of 100 | 0.349870 @ 25.501705%\n",
      "  1 x  95 Layers with 0.000000 dropout and batch size of 150 | 0.351897 @ 22.657777%\n",
      "  1 x  95 Layers with 0.000000 dropout and batch size of 200 | 0.363180 @ 21.172781%\n",
      "  1 x  95 Layers with 0.000000 dropout and batch size of 250 | 0.364192 @ 20.935864%\n",
      "  1 x  95 Layers with 0.000000 dropout and batch size of 300 | 0.370766 @ 20.472739%\n",
      "  1 x  95 Layers with 0.000000 dropout and batch size of 350 | 0.411293 @ 15.675596%\n",
      "  1 x  95 Layers with 0.100000 dropout and batch size of  50 | 0.353793 @ 23.086204%\n",
      "  1 x  95 Layers with 0.100000 dropout and batch size of 100 | 0.372554 @ 17.734234%\n",
      "  1 x  95 Layers with 0.100000 dropout and batch size of 150 | 0.369229 @ 19.803841%\n",
      "  1 x  95 Layers with 0.100000 dropout and batch size of 200 | 0.375853 @ 18.510130%\n",
      "  1 x  95 Layers with 0.100000 dropout and batch size of 250 | 0.390973 @ 15.029508%\n",
      "  1 x  95 Layers with 0.100000 dropout and batch size of 300 | 0.390038 @ 16.899899%\n",
      "  1 x  95 Layers with 0.100000 dropout and batch size of 350 | 0.431646 @ 13.474891%\n",
      "  1 x  95 Layers with 0.200000 dropout and batch size of  50 | 0.353071 @ 22.215267%\n",
      "  1 x  95 Layers with 0.200000 dropout and batch size of 100 | 0.376845 @ 17.545956%\n",
      "  1 x  95 Layers with 0.200000 dropout and batch size of 150 | 0.379839 @ 17.973259%\n",
      "  1 x  95 Layers with 0.200000 dropout and batch size of 200 | 0.397419 @ 14.420816%\n",
      "  1 x  95 Layers with 0.200000 dropout and batch size of 250 | 0.393262 @ 15.771006%\n",
      "  1 x  95 Layers with 0.200000 dropout and batch size of 300 | 0.400163 @ 14.825328%\n",
      "  1 x  95 Layers with 0.200000 dropout and batch size of 350 | 0.438910 @ 11.478914%\n",
      "  1 x  95 Layers with 0.300000 dropout and batch size of  50 | 0.377124 @ 17.259513%\n",
      "  1 x  95 Layers with 0.300000 dropout and batch size of 100 | 0.402569 @ 12.695241%\n",
      "  1 x  95 Layers with 0.300000 dropout and batch size of 150 | 0.392808 @ 14.947567%\n",
      "  1 x  95 Layers with 0.300000 dropout and batch size of 200 | 0.402508 @ 13.256969%\n",
      "  1 x  95 Layers with 0.300000 dropout and batch size of 250 | 0.402891 @ 13.877689%\n",
      "  1 x  95 Layers with 0.300000 dropout and batch size of 300 | 0.408271 @ 13.422704%\n",
      "  1 x  95 Layers with 0.300000 dropout and batch size of 350 | 0.437631 @ 11.785081%\n",
      "  1 x  95 Layers with 0.400000 dropout and batch size of  50 | 0.387866 @ 15.967161%\n",
      "  1 x  95 Layers with 0.400000 dropout and batch size of 100 | 0.388836 @ 15.637976%\n",
      "  1 x  95 Layers with 0.400000 dropout and batch size of 150 | 0.401156 @ 13.092875%\n",
      "  1 x  95 Layers with 0.400000 dropout and batch size of 200 | 0.414906 @ 11.701000%\n",
      "  1 x  95 Layers with 0.400000 dropout and batch size of 250 | 0.410606 @ 13.379208%\n",
      "  1 x  95 Layers with 0.400000 dropout and batch size of 300 | 0.419326 @ 11.117505%\n",
      "  1 x  95 Layers with 0.400000 dropout and batch size of 350 | 0.448684 @ 9.151663%\n",
      "  1 x  95 Layers with 0.500000 dropout and batch size of  50 | 0.389749 @ 14.590832%\n",
      "  1 x  95 Layers with 0.500000 dropout and batch size of 100 | 0.398423 @ 15.096578%\n",
      "  1 x  95 Layers with 0.500000 dropout and batch size of 150 | 0.424601 @ 9.616482%\n",
      "  1 x  95 Layers with 0.500000 dropout and batch size of 200 | 0.413132 @ 11.925570%\n",
      "  1 x  95 Layers with 0.500000 dropout and batch size of 250 | 0.426921 @ 9.310843%\n",
      "  1 x  95 Layers with 0.500000 dropout and batch size of 300 | 0.425599 @ 10.386981%\n",
      "  1 x  95 Layers with 0.500000 dropout and batch size of 350 | 0.442672 @ 9.398802%\n",
      "  1 x  95 Layers with 0.600000 dropout and batch size of  50 | 0.396702 @ 14.344479%\n",
      "  1 x  95 Layers with 0.600000 dropout and batch size of 100 | 0.423365 @ 9.438506%\n",
      "  1 x  95 Layers with 0.600000 dropout and batch size of 150 | 0.416727 @ 11.434691%\n",
      "  1 x  95 Layers with 0.600000 dropout and batch size of 200 | 0.427491 @ 9.577207%\n",
      "  1 x  95 Layers with 0.600000 dropout and batch size of 250 | 0.422029 @ 11.379657%\n",
      "  1 x  95 Layers with 0.600000 dropout and batch size of 300 | 0.438939 @ 7.428774%\n",
      "  1 x  95 Layers with 0.600000 dropout and batch size of 350 | 0.460045 @ 5.416110%\n",
      "  1 x  95 Layers with 0.700000 dropout and batch size of  50 | 0.404335 @ 13.769242%\n",
      "  1 x  95 Layers with 0.700000 dropout and batch size of 100 | 0.427746 @ 8.749003%\n",
      "  1 x  95 Layers with 0.700000 dropout and batch size of 150 | 0.435031 @ 8.033102%\n",
      "  1 x  95 Layers with 0.700000 dropout and batch size of 200 | 0.430367 @ 9.087692%\n",
      "  1 x  95 Layers with 0.700000 dropout and batch size of 250 | 0.468568 @ 2.364980%\n",
      "  1 x  95 Layers with 0.700000 dropout and batch size of 300 | 0.436968 @ 8.629791%\n",
      "  1 x  95 Layers with 0.700000 dropout and batch size of 350 | 0.464063 @ 5.884279%\n",
      "  1 x  95 Layers with 0.800000 dropout and batch size of  50 | 0.429069 @ 8.480923%\n",
      "  1 x  95 Layers with 0.800000 dropout and batch size of 100 | 0.443274 @ 6.920682%\n",
      "  1 x  95 Layers with 0.800000 dropout and batch size of 150 | 0.445721 @ 6.150609%\n",
      "  1 x  95 Layers with 0.800000 dropout and batch size of 200 | 0.467257 @ 2.738597%\n",
      "  1 x  95 Layers with 0.800000 dropout and batch size of 250 | 0.470553 @ 2.164250%\n",
      "  1 x  95 Layers with 0.800000 dropout and batch size of 300 | 0.452230 @ 6.075588%\n",
      "  1 x  95 Layers with 0.800000 dropout and batch size of 350 | 0.481114 @ 4.703573%\n",
      "  1 x  95 Layers with 0.900000 dropout and batch size of  50 | 0.450578 @ 5.256527%\n",
      "  1 x  95 Layers with 0.900000 dropout and batch size of 100 | 0.487483 @ 1.905458%\n",
      "  1 x  95 Layers with 0.900000 dropout and batch size of 150 | 0.476107 @ 0.762990%\n",
      "  1 x  95 Layers with 0.900000 dropout and batch size of 200 | 0.477379 @ 1.088169%\n",
      "  1 x  95 Layers with 0.900000 dropout and batch size of 250 | 0.494951 @ 1.383046%\n",
      "  1 x  95 Layers with 0.900000 dropout and batch size of 300 | 0.508082 @ 3.075186%\n",
      "  1 x  95 Layers with 0.900000 dropout and batch size of 350 | 0.523744 @ 0.866170%\n",
      "  1 x 125 Layers with 0.000000 dropout and batch size of  50 | 0.327828 @ 30.220907%\n",
      "  1 x 125 Layers with 0.000000 dropout and batch size of 100 | 0.351961 @ 24.527755%\n",
      "  1 x 125 Layers with 0.000000 dropout and batch size of 150 | 0.349912 @ 23.322005%\n",
      "  1 x 125 Layers with 0.000000 dropout and batch size of 200 | 0.357771 @ 21.542104%\n",
      "  1 x 125 Layers with 0.000000 dropout and batch size of 250 | 0.376787 @ 18.271207%\n",
      "  1 x 125 Layers with 0.000000 dropout and batch size of 300 | 0.388203 @ 18.471215%\n",
      "  1 x 125 Layers with 0.000000 dropout and batch size of 350 | 0.423641 @ 17.533994%\n",
      "  1 x 125 Layers with 0.100000 dropout and batch size of  50 | 0.349384 @ 25.965307%\n",
      "  1 x 125 Layers with 0.100000 dropout and batch size of 100 | 0.369513 @ 19.544556%\n",
      "  1 x 125 Layers with 0.100000 dropout and batch size of 150 | 0.371902 @ 18.311626%\n",
      "  1 x 125 Layers with 0.100000 dropout and batch size of 200 | 0.382068 @ 17.629891%\n",
      "  1 x 125 Layers with 0.100000 dropout and batch size of 250 | 0.380531 @ 17.944860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 x 125 Layers with 0.100000 dropout and batch size of 300 | 0.388067 @ 15.780122%\n",
      "  1 x 125 Layers with 0.100000 dropout and batch size of 350 | 0.420619 @ 12.798477%\n",
      "  1 x 125 Layers with 0.200000 dropout and batch size of  50 | 0.356542 @ 21.865169%\n",
      "  1 x 125 Layers with 0.200000 dropout and batch size of 100 | 0.368077 @ 19.031383%\n",
      "  1 x 125 Layers with 0.200000 dropout and batch size of 150 | 0.389780 @ 13.868122%\n",
      "  1 x 125 Layers with 0.200000 dropout and batch size of 200 | 0.392351 @ 15.414750%\n",
      "  1 x 125 Layers with 0.200000 dropout and batch size of 250 | 0.396463 @ 14.628902%\n",
      "  1 x 125 Layers with 0.200000 dropout and batch size of 300 | 0.397212 @ 13.913770%\n",
      "  1 x 125 Layers with 0.200000 dropout and batch size of 350 | 0.437295 @ 11.941641%\n",
      "  1 x 125 Layers with 0.300000 dropout and batch size of  50 | 0.376884 @ 18.229900%\n",
      "  1 x 125 Layers with 0.300000 dropout and batch size of 100 | 0.404931 @ 13.017036%\n",
      "  1 x 125 Layers with 0.300000 dropout and batch size of 150 | 0.382668 @ 15.963180%\n",
      "  1 x 125 Layers with 0.300000 dropout and batch size of 200 | 0.385679 @ 16.423873%\n",
      "  1 x 125 Layers with 0.300000 dropout and batch size of 250 | 0.412444 @ 11.108995%\n",
      "  1 x 125 Layers with 0.300000 dropout and batch size of 300 | 0.416739 @ 11.505636%\n",
      "  1 x 125 Layers with 0.300000 dropout and batch size of 350 | 0.422554 @ 12.871884%\n",
      "  1 x 125 Layers with 0.400000 dropout and batch size of  50 | 0.383494 @ 16.322041%\n",
      "  1 x 125 Layers with 0.400000 dropout and batch size of 100 | 0.391600 @ 15.333864%\n",
      "  1 x 125 Layers with 0.400000 dropout and batch size of 150 | 0.388638 @ 15.608697%\n",
      "  1 x 125 Layers with 0.400000 dropout and batch size of 200 | 0.408736 @ 12.527507%\n",
      "  1 x 125 Layers with 0.400000 dropout and batch size of 250 | 0.403183 @ 14.841345%\n",
      "  1 x 125 Layers with 0.400000 dropout and batch size of 300 | 0.420218 @ 11.041860%\n",
      "  1 x 125 Layers with 0.400000 dropout and batch size of 350 | 0.439618 @ 9.927066%\n",
      "  1 x 125 Layers with 0.500000 dropout and batch size of  50 | 0.404187 @ 11.681592%\n",
      "  1 x 125 Layers with 0.500000 dropout and batch size of 100 | 0.404209 @ 12.562877%\n",
      "  1 x 125 Layers with 0.500000 dropout and batch size of 150 | 0.400333 @ 14.055579%\n",
      "  1 x 125 Layers with 0.500000 dropout and batch size of 200 | 0.411885 @ 12.031585%\n"
     ]
    }
   ],
   "source": [
    "layersGrid = np.arange(1,7)\n",
    "layerSizeGrid = np.arange(5,200,30)\n",
    "dropoutGrid = np.arange(0,1,0.10)\n",
    "batchGrid = np.arange(50,400,50)\n",
    "\n",
    "res = gridSearch(layersGrid,layerSizeGrid,dropoutGrid,batchGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7a70bace0e39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlowestRes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfindBestParams_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfindBestParams_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "def findBestParams_1(res):\n",
    "    lowestLoss = 10000\n",
    "    lowestDiff = 10000\n",
    "    lowestRes = None\n",
    "\n",
    "    for result in res:\n",
    "        hist = res[result]\n",
    "        loss = hist.history['loss'][-1]\n",
    "        val_loss = hist.history['val_loss'][-1]\n",
    "        diff = np.abs((1 - (loss/val_loss))*100)\n",
    "\n",
    "        if(diff<lowestDiff and loss < lowestLoss):\n",
    "            lowestDiff = diff\n",
    "            lowestLoss = loss\n",
    "            lowestRes = result\n",
    "            \n",
    "    return lowestRes\n",
    "\n",
    "def findBestParams_2(res):\n",
    "    lowestLoss = 10000\n",
    "    lowestRes = None\n",
    "\n",
    "    for result in res:\n",
    "        hist = res[result]\n",
    "        val_loss = hist.history['val_loss'][-1]\n",
    "\n",
    "        if(val_loss < lowestLoss):\n",
    "            lowestLoss = val_loss\n",
    "            lowestRes = result\n",
    "            \n",
    "    return lowestRes\n",
    "\n",
    "print(findBestParams_1(res))\n",
    "print(findBestParams_2(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c42dbe90324865aef6d2af6243b330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description='layers', options=(1, 2, 3), value=1), Dropdown(description='layerSize', options=(5, 35, 65, 95, 125, 155, 185), value=5), Dropdown(description='dropout', options=(0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9), value=0.0), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visGridSearch>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def learningCurves(hist):\n",
    "    histAcc_train = hist.history['acc']\n",
    "    histLoss_train = hist.history['loss']\n",
    "    histAcc_validation = hist.history['val_acc']\n",
    "    histLoss_validation = hist.history['val_loss']\n",
    "    maxValAcc = np.max(histAcc_validation)\n",
    "    minValLoss = np.min(histLoss_validation)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "\n",
    "    plt.plot(range(epochs),np.full(epochs,meanBaseline(trainDF)),label=\"Unbiased Estimator\", color=\"red\")\n",
    "\n",
    "    plt.plot(range(epochs),histLoss_train, label=\"Training Loss\", color=\"#acc6ef\")\n",
    "    plt.plot(range(epochs),histAcc_train, label=\"Training Accuracy\", color = \"#005ff9\" )\n",
    "\n",
    "    plt.plot(range(epochs),histLoss_validation, label=\"Validation Loss\", color=\"#a7e295\")\n",
    "    plt.plot(range(epochs),histAcc_validation, label=\"Validation Accuracy\",color=\"#3ddd0d\")\n",
    "\n",
    "    plt.scatter(np.argmax(histAcc_validation),maxValAcc,zorder=10,color=\"green\")\n",
    "    plt.scatter(np.argmin(histLoss_validation),minValLoss,zorder=10,color=\"green\")\n",
    "\n",
    "    plt.xlabel('Epochs',fontsize=14)\n",
    "    plt.title(\"Learning Curves\",fontsize=20)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Max validation accuracy: {0}\".format(maxValAcc))\n",
    "    print(\"Minimum validation loss: {0}\".format(minValLoss))\n",
    "\n",
    "def visGridSearch(layers,layerSize,dropout):\n",
    "    learningCurves(res[(layers,layerSize,dropout,lossFn,optimizer,batch_size,epochs)])\n",
    "\n",
    "interact(visGridSearch,layers=layersGrid,layerSize=layerSizeGrid,dropout=dropoutGrid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
